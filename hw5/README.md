# STM32 Embedded AI: Audio & Image Classification Projects

This repository contains two distinct implementation of embedded machine learning applications on an STM32 microcontroller using **X-CUBE-AI**. The projects demonstrate the complete pipeline from data preprocessing and model training in Python (TensorFlow/Keras) to deployment on embedded hardware.

## Projects Overview

1.  **Audio Classification:** Spoken Digit Recognition using MFCC features.
2.  **Image Classification:** MNIST Digit Recognition using Hu Moments (Feature Extraction).

---

## ðŸ›  Technologies & Tools

* **Hardware:** STM32 Microcontroller
* **Software:** STM32CubeIDE, STM32CubeMX, X-CUBE-AI
* **ML Framework:** TensorFlow, Keras, Scikit-learn
* **Language:** Python (Training), C (Inference)
* **Signal Processing:** Librosa/Scipy (Audio), OpenCV (Image)

---

## 1. Audio Classification: Spoken Digit Recognition

This project classifies spoken digits (0-9) using the Free Spoken Digit Dataset (FSDD). Instead of raw audio, we use Mel-frequency cepstral coefficients (MFCC) for feature extraction to reduce computational load on the MCU.

### ðŸ”¹ Methodology
* **Input:** `.wav` audio files (8kHz sample rate).
* **Feature Extraction:** * FFT Size: 1024
    * Mel Filters: 20
    * DCT Outputs: 13 (Total 26 inputs: 13 MFCC + 13 Delta).
* **Model Architecture:** MLP (Dense Neural Network)
    * Input (26) -> Dense(100, ReLU) -> Dense(100, ReLU) -> Output(10, Softmax).

### ðŸ“Š Results
The model was trained for 100 epochs. Below is the confusion matrix showing the prediction performance on the test set:

![Audio Confusion Matrix](https://github.com/user-attachments/assets/0eaa6c0b-fbb7-48e6-9928-45bcb72d36db)

### ðŸ“‚ Key Files (Audio)
* `train_keyword_spotting.py`: Extracts MFCC features, trains the MLP model, and saves as `.h5`.
* `create_real_input.py`: Converts a real `.wav` file into a C header file (`test_input.h`) containing the MFCC array for testing on STM32.

---

## 2. Image Classification: MNIST with Hu Moments

This project classifies handwritten digits from the MNIST dataset. To make the model lightweight for embedded systems, we do not use raw pixels (28x28). Instead, we extract **Hu Moments** (7 invariant features) using OpenCV.

### ðŸ”¹ Methodology
* **Input:** MNIST Images.
* **Feature Extraction:** * Calculated **Hu Moments** (7 values per image).
    * **Normalization:** Features are normalized (Mean/Std) to improve convergence.
* **Model Architecture:** MLP
    * Input (7) -> Dense(100, ReLU) -> Dense(100, ReLU) -> Output(10, Softmax).

### ðŸ“Š Results
The model achieved high accuracy using only 7 input features.

![Image Confusion Matrix](https://github.com/user-attachments/assets/e6d36028-9f3f-46c4-b318-325310b4e2b0)

### ðŸ“‚ Key Files (Image)
* `h5_to_c.py`: Loads the trained model, calculates Mean/Std from training data, and generates `hw5_q2_data.h` containing weights and normalization parameters for C code.
* `test_data_generate.py`: Extracts samples from the MNIST test set and generates `hw5_q2_test_data.h` for on-device verification.

---

## ðŸš€ STM32 Deployment Workflow

The deployment process follows these steps for both projects:

1.  **Train Model:** Train the Keras model (`.h5`) in Python.
2.  **Convert to TFLite:** Use `tf.lite.TFLiteConverter` to create a `.tflite` file.
3.  **X-CUBE-AI Integration:** Import the `.tflite` model into the STM32 project using the X-CUBE-AI plugin.
4.  **C Code Integration:**
    * Include the generated header files for test inputs.
    * Modify `app_x-cube-ai.c` to load data into the input buffer.
    * Run inference (`ai_run`) and parse the output buffer.

### C Implementation Snippet (`app_x-cube-ai.c`)

The inference logic inside the microcontroller:

```c
// 1. Load Data (from generated header)
for (int i = 0; i < INPUT_SIZE; i++) {
    in_data[i] = (ai_float)test_input_data[i];
}

// 2. Run Inference
ai_run();

// 3. Process Output (Argmax)
float max_prob = 0.0f;
int predicted_digit = -1;

for (int i = 0; i < 10; i++) {
    if (out_data[i] > max_prob) {
        max_prob = out_data[i];
        predicted_digit = i;
    }
}




////////////////











# EE 4065 - Embedded Digital Image Processing: Homework 5

**Course:** EE 4065 - Embedded Digital Image Processing
**Assignment:** Homework 5

## Student Information

| Name | Student ID |
| :--- | :--- |
| Yusuf OruÃ§ | 150720036 |
| Alihan Kocaakman | 150720065 |

## Q1

### Result (Q1) Predicted Digit and its probability 
<img width="499" height="187" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2026-01-01 230803" src="https://github.com/user-attachments/assets/0eaa6c0b-fbb7-48e6-9928-45bcb72d36db" />

### Training Code

```python
import os
import numpy as np
import scipy.signal as sig
from mfcc_func import create_mfcc_features # Az Ã¶nce oluÅŸturduÄŸumuz dosya
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import tensorflow as tf
from matplotlib import pyplot as plt
from sklearn.preprocessing import OneHotEncoder

# Veri seti klasÃ¶rÃ¼
RECORDINGS_DIR = "recordings"

# KlasÃ¶r kontrolÃ¼
if not os.path.exists(RECORDINGS_DIR):
    print("HATA: 'recordings' klasÃ¶rÃ¼ bulunamadÄ±. LÃ¼tfen oluÅŸturup iÃ§ine .wav dosyalarÄ±nÄ± atÄ±n.")
    exit()

# TÃ¼m .wav dosyalarÄ±nÄ± listele
recordings_list = [(RECORDINGS_DIR, f) for f in os.listdir(RECORDINGS_DIR) if f.endswith(".wav")]

# Parametreler (STM32 tarafÄ±nda C kodu ile uyumlu olmalÄ±)
FFTSize = 1024
sample_rate = 8000
numOfMelFilters = 20
numOfDctOutputs = 13  # MFCC katsayÄ± sayÄ±sÄ±

# Test ve EÄŸitim setlerini ayÄ±rma (Kitaptaki gibi 'yweweler' test iÃ§in ayrÄ±lÄ±yor)
test_list = {record for record in recordings_list if "yweweler" in record[1]}
train_list = set(recordings_list) - test_list

print("EÄŸitim verisi hazÄ±rlanÄ±yor (Bu iÅŸlem biraz sÃ¼rebilir)...")
train_mfcc_features, train_labels = create_mfcc_features(list(train_list), FFTSize, sample_rate, numOfMelFilters, numOfDctOutputs)

print("Test verisi hazÄ±rlanÄ±yor...")
test_mfcc_features, test_labels = create_mfcc_features(list(test_list), FFTSize, sample_rate, numOfMelFilters, numOfDctOutputs)

# Model Mimarisi [cite: 11]
# GiriÅŸ katmanÄ±: 26 nÃ¶ron (13 MFCC + 13 Delta)
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(26,)), 
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(10, activation="softmax") # 0-9 arasÄ± rakamlar iÃ§in Ã§Ä±kÄ±ÅŸ
])

# Etiketleri One-Hot formatÄ±na Ã§evirme
ohe = OneHotEncoder()
train_labels_ohe = ohe.fit_transform(train_labels.reshape(-1, 1)).toarray()

# Modeli Derleme
model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), 
              optimizer=tf.keras.optimizers.Adam(1e-3), 
              metrics=['accuracy'])

# EÄŸitimi BaÅŸlat
model.fit(train_mfcc_features, train_labels_ohe, epochs=100, verbose=1)

# Test SonuÃ§larÄ±nÄ± GÃ¶ster
nn_preds = model.predict(test_mfcc_features)
predicted_classes = np.argmax(nn_preds, axis=1)

# Confusion Matrix
categories = np.unique(test_labels)
conf_matrix = confusion_matrix(test_labels, predicted_classes)
cm_display = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=categories)
cm_display.plot()
plt.title("Neural Network Confusion Matrix")
plt.show()

# MODELÄ° KAYDET (En Ã–nemli AdÄ±m)
model.save("mlp_fsdd_model.h5")
print("Model baÅŸarÄ±yla 'mlp_fsdd_model.h5' olarak kaydedildi.")
```
---

### conversion file from h5 to tflite

```python
import tensorflow as tf
import os

# Model dosya yolunu kontrol et
model_path = "mlp_fsdd_model.h5"

if not os.path.exists(model_path):
    print("HATA: .h5 dosyasÄ± bulunamadÄ±!")
    exit()

try:
    # 1. Mevcut modeli yÃ¼kle
    model = tf.keras.models.load_model(model_path)
    
    # 2. TFLite DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ oluÅŸtur
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    
    # (Opsiyonel) STM32 iÃ§in optimizasyonlarÄ± aÃ§ar
    # converter.optimizations = [tf.lite.Optimize.DEFAULT] 
    
    # 3. DÃ¶nÃ¼ÅŸtÃ¼r
    tflite_model = converter.convert()

    # 4. Kaydet (.tflite olarak)
    tflite_path = "mlp_fsdd_model.tflite"
    with open(tflite_path, 'wb') as f:
        f.write(tflite_model)
        
    print(f"BaÅŸarÄ±lÄ±! Model '{tflite_path}' olarak kaydedildi.")
    print("Åžimdi STM32CubeMX'te bu .tflite dosyasÄ±nÄ± seÃ§.")

except Exception as e:
    print(f"DÃ¶nÃ¼ÅŸtÃ¼rme HatasÄ±: {e}")
```
---


### tflite to test_input_h

import os

# Dosya isimlerini tanÄ±mla
tflite_path = 'mlp_fsdd_model.tflite'
output_header_path = 'model_data.h'
array_name = 'mlp_fsdd_model_tflite'

# Binary dosyayÄ± oku
with open(tflite_path, 'rb') as f:
    data = f.read()

# C header dosyasÄ±nÄ± yaz
with open(output_header_path, 'w') as f:
    f.write(f'// Bu dosya Python scripti ile otomatik oluÅŸturuldu.\n\n')
    f.write(f'const unsigned char {array_name}[] = {{\n')
    
    for i, byte in enumerate(data):
        f.write(f'0x{byte:02x}, ')
        if (i + 1) % 12 == 0: # Okunabilirlik iÃ§in her 12 byteda bir alt satÄ±ra geÃ§
            f.write('\n')
            
    f.write('};\n\n')
    f.write(f'const unsigned int {array_name}_len = {len(data)};\n')

print(f"{output_header_path} baÅŸarÄ±yla oluÅŸturuldu!")


app_c-cube-ai.c


/**
  ******************************************************************************
  * @file    app_x-cube-ai.c
  * @author  X-CUBE-AI C code generator
  * @brief   AI program body
  ******************************************************************************
  * @attention
  *
  * Copyright (c) 2026 STMicroelectronics.
  * All rights reserved.
  *
  * This software is licensed under terms that can be found in the LICENSE file
  * in the root directory of this software component.
  * If no LICENSE file comes with this software, it is provided AS-IS.
  *
  ******************************************************************************
  */

 /*
  * Description
  *   v1.0 - Minimum template to show how to use the Embedded Client API
  *          model. Only one input and one output is supported. All
  *          memory resources are allocated statically (AI_NETWORK_XX, defines
  *          are used).
  *          Re-target of the printf function is out-of-scope.
  *   v2.0 - add multiple IO and/or multiple heap support
  *
  *   For more information, see the embeded documentation:
  *
  *       [1] %X_CUBE_AI_DIR%/Documentation/index.html
  *
  *   X_CUBE_AI_DIR indicates the location where the X-CUBE-AI pack is installed
  *   typical : C:\Users\[user_name]\STM32Cube\Repository\STMicroelectronics\X-CUBE-AI\7.1.0
  */

#ifdef __cplusplus
 extern "C" {
#endif

/* Includes ------------------------------------------------------------------*/

#if defined ( __ICCARM__ )
#elif defined ( __CC_ARM ) || ( __GNUC__ )
#endif

/* System headers */
#include <stdint.h>
#include <stdlib.h>
#include <stdio.h>
#include <inttypes.h>
#include <string.h>

#include "app_x-cube-ai.h"
#include "main.h"
#include "ai_datatypes_defines.h"
#include "hw5_last.h"
#include "hw5_last_data.h"

/* USER CODE BEGIN includes */
 /* USER CODE BEGIN Includes */
 #include <test_input_audio.h>  // Python ile oluÅŸturduÄŸumuz MFCC verisi
 #include <stdio.h>       // Gerekirse printf iÃ§in

 /* USER CODE BEGIN 0 */
 // Debug iÃ§in Global DeÄŸiÅŸkenler
 float max_prob = 0.0f;
 int predicted_digit = -1;
 /* USER CODE END 0 */
 /* USER CODE END Includes */
/* USER CODE END includes */

/* IO buffers ----------------------------------------------------------------*/

#if !defined(AI_HW5_LAST_INPUTS_IN_ACTIVATIONS)
AI_ALIGNED(4) ai_i8 data_in_1[AI_HW5_LAST_IN_1_SIZE_BYTES];
ai_i8* data_ins[AI_HW5_LAST_IN_NUM] = {
data_in_1
};
#else
ai_i8* data_ins[AI_HW5_LAST_IN_NUM] = {
NULL
};
#endif

#if !defined(AI_HW5_LAST_OUTPUTS_IN_ACTIVATIONS)
AI_ALIGNED(4) ai_i8 data_out_1[AI_HW5_LAST_OUT_1_SIZE_BYTES];
ai_i8* data_outs[AI_HW5_LAST_OUT_NUM] = {
data_out_1
};
#else
ai_i8* data_outs[AI_HW5_LAST_OUT_NUM] = {
NULL
};
#endif

/* Activations buffers -------------------------------------------------------*/

AI_ALIGNED(32)
static uint8_t pool0[AI_HW5_LAST_DATA_ACTIVATION_1_SIZE];

ai_handle data_activations0[] = {pool0};

/* AI objects ----------------------------------------------------------------*/

static ai_handle hw5_last = AI_HANDLE_NULL;

static ai_buffer* ai_input;
static ai_buffer* ai_output;

static void ai_log_err(const ai_error err, const char *fct)
{
  /* USER CODE BEGIN log */
  /*if (fct)
   printf("TEMPLATE - Error (%s) - type=0x%02x code=0x%02x\r\n", fct,
        err.type, err.code);
  else
    printf("TEMPLATE - Error - type=0x%02x code=0x%02x\r\n", err.type, err.code);
*/
  do {} while (1);
  /* USER CODE END log */
}

static int ai_boostrap(ai_handle *act_addr)
{
  ai_error err;

  /* Create and initialize an instance of the model */
  err = ai_hw5_last_create_and_init(&hw5_last, act_addr, NULL);
  if (err.type != AI_ERROR_NONE) {
    ai_log_err(err, "ai_hw5_last_create_and_init");
    return -1;
  }

  ai_input = ai_hw5_last_inputs_get(hw5_last, NULL);
  ai_output = ai_hw5_last_outputs_get(hw5_last, NULL);

#if defined(AI_HW5_LAST_INPUTS_IN_ACTIVATIONS)
  /*  In the case where "--allocate-inputs" option is used, memory buffer can be
   *  used from the activations buffer. This is not mandatory.
   */
  for (int idx=0; idx < AI_HW5_LAST_IN_NUM; idx++) {
	data_ins[idx] = ai_input[idx].data;
  }
#else
  for (int idx=0; idx < AI_HW5_LAST_IN_NUM; idx++) {
	  ai_input[idx].data = data_ins[idx];
  }
#endif

#if defined(AI_HW5_LAST_OUTPUTS_IN_ACTIVATIONS)
  /*  In the case where "--allocate-outputs" option is used, memory buffer can be
   *  used from the activations buffer. This is no mandatory.
   */
  for (int idx=0; idx < AI_HW5_LAST_OUT_NUM; idx++) {
	data_outs[idx] = ai_output[idx].data;
  }
#else
  for (int idx=0; idx < AI_HW5_LAST_OUT_NUM; idx++) {
	ai_output[idx].data = data_outs[idx];
  }
#endif

  return 0;
}

static int ai_run(void)
{
  ai_i32 batch;

  batch = ai_hw5_last_run(hw5_last, ai_input, ai_output);
  if (batch != 1) {
    ai_log_err(ai_hw5_last_get_error(hw5_last),
        "ai_hw5_last_run");
    return -1;
  }

  return 0;
}

/* USER CODE BEGIN 2 */
int acquire_and_process_data(ai_i8* data[])
{
  /* fill the inputs of the c-model
  for (int idx=0; idx < AI_HW5_LAST_IN_NUM; idx++ )
  {
      data[idx] = ....
  }

  */
  return 0;
}

int post_process(ai_i8* data[])
{
  /* process the predictions
  for (int idx=0; idx < AI_HW5_LAST_OUT_NUM; idx++ )
  {
      data[idx] = ....
  }

  */
  return 0;
}
/* USER CODE END 2 */

/* Entry points --------------------------------------------------------------*/

void MX_X_CUBE_AI_Init(void)
{
    /* USER CODE BEGIN 5 */
  //printf("\r\nTEMPLATE - initialization\r\n");

  ai_boostrap(data_activations0);
    /* USER CODE END 5 */
}

void MX_X_CUBE_AI_Process(void)
{
    /* USER CODE BEGIN 6 */
  int res = -1;

  //printf("TEMPLATE - run - main loop\r\n");

  if (hw5_last) {

      // --- BÄ°ZÄ°M EKLEDÄ°ÄžÄ°MÄ°Z KISIM BAÅžLIYOR ---

      // 1. Buffer EriÅŸimleri
      ai_float *in_data = (ai_float *)ai_input[0].data;
      ai_float *out_data = (ai_float *)ai_output[0].data;

      // 2. Test Verisini YÃ¼kle
      //printf("Test verisi yukleniyor...\r\n");
      for (int i = 0; i < 26; i++) {
          in_data[i] = (ai_float)test_input_mfcc[i];
      }

      // 3. Modeli Ã‡alÄ±ÅŸtÄ±r
      //printf("Model calistiriliyor (Inference)...\r\n");
      res = ai_run();

      // 4. Sonucu Analiz Et
      if (res == 0) {
          // NOT: Buradaki "float" ve "int" kelimelerini sildik Ã§Ã¼nkÃ¼
          // yukarÄ±da global olarak tanÄ±mladÄ±k. Sadece deÄŸerlerini sÄ±fÄ±rlÄ±yoruz.
          max_prob = 0.0f;
          predicted_digit = -1;

          for (int i = 0; i < 10; i++) {
              if (out_data[i] > max_prob) {
                  max_prob = out_data[i];
                  predicted_digit = i;
              }
          }

          // Konsol Ã‡Ä±ktÄ±larÄ±
         // printf("--------------------------------\r\n");
         // printf("TAHMIN EDILEN RAKAM: %d\r\n", predicted_digit);
         // printf("GUVEN ORANI: %f\r\n", max_prob);
         // printf("--------------------------------\r\n");
      }
      else {
         // printf("Hata kodu: %d\r\n", res);
      }

      HAL_Delay(2000);
      // --- BÄ°ZÄ°M KISIM BÄ°TTÄ° ---
  }

  if (res) {
    ai_error err = {AI_ERROR_INVALID_STATE, AI_ERROR_CODE_NETWORK};
    ai_log_err(err, "Process has FAILED");
  }
    /* USER CODE END 6 */
}
#ifdef __cplusplus
}
#endif


create_real_input.py

import numpy as np
import os
# Senin eÄŸitimde kullandÄ±ÄŸÄ±n fonksiyonu Ã§aÄŸÄ±rÄ±yoruz
from mfcc_func import create_mfcc_features 

# --- AYARLAR ---
# Test etmek istediÄŸin dosyanÄ±n tam adÄ± (KlasÃ¶rde bu dosya olmalÄ±!)
# Model 0-9 arasÄ± rakamlarÄ± bildiÄŸi iÃ§in "0" ile baÅŸlayan bir dosya seÃ§tik.
TEST_FILE_DIR = "recordings"
TEST_FILE_NAME = "0_jackson_0.wav" 

# EÄŸitimdeki parametrelerin AYNISI olmalÄ± (Listing 11.5'ten alÄ±ndÄ±)
FFT_SIZE = 1024
SAMPLE_RATE = 8000
NUM_OF_MEL_FILTERS = 20
NUM_OF_DCT_OUTPUTS = 13

def generate_header():
    file_path = os.path.join(TEST_FILE_DIR, TEST_FILE_NAME)
    
    if not os.path.exists(file_path):
        print(f"HATA: '{file_path}' dosyasi bulunamadi! Lutfen dosya yolunu kontrol et.")
        return

    print(f"Islem basliyor: {TEST_FILE_NAME} dosyasi isleniyor...")

    # create_mfcc_features fonksiyonu genelde bir liste bekler.
    # Bizim elimizde tek dosya var, o yÃ¼zden onu tek elemanlÄ± bir liste gibi gÃ¶steriyoruz.
    # YapÄ±: [(KlasÃ¶rYolu, DosyaAdi)]
    single_file_list = [(TEST_FILE_DIR, TEST_FILE_NAME)]

    # Fonksiyonu Ã§aÄŸÄ±rarak MFCC Ã¶zelliklerini Ã§Ä±karÄ±yoruz
    # Bu fonksiyon hem Ã¶zellikleri (features) hem etiketleri (labels) dÃ¶ndÃ¼rÃ¼r.
    features, labels = create_mfcc_features(
        single_file_list, 
        FFT_SIZE, 
        SAMPLE_RATE, 
        NUM_OF_MEL_FILTERS, 
        NUM_OF_DCT_OUTPUTS
    )

    # features deÄŸiÅŸkeni muhtemelen [[...26 deÄŸer...]] ÅŸeklinde bir listedir.
    # Ä°lk (ve tek) elemanÄ± alÄ±yoruz.
    real_mfcc_input = features[0]

    # --- KONTROL ---
    print(f"Elde edilen ozellik boyutu: {len(real_mfcc_input)}")
    if len(real_mfcc_input) != 26:
        print("HATA: Model 26 giris bekliyor ama fonksiyon farkli sayida uretti!")
        return

    # --- C HEADER DOSYASINI YAZMA ---
    header_content = f"""
// Bu dosya create_real_input.py tarafindan olusturuldu.
// Kaynak ses dosyasi: {TEST_FILE_NAME}
// Beklenen Tahmin: 0 (Sifir)

#ifndef TEST_INPUT_H
#define TEST_INPUT_H

// Yapay zeka modelinin girisine kopyalanacak 26 adet MFCC degeri:
const float test_input_mfcc[26] = {{
"""
    
    # SayÄ±larÄ± virgÃ¼le ayÄ±rarak yaz
    count = 0
    for val in real_mfcc_input:
        header_content += f"    {val:.6f}f, "
        count += 1
        if count % 5 == 0: # Okunabilirlik iÃ§in satÄ±r atla
            header_content += "\n"

    header_content += "\n};\n\n#endif // TEST_INPUT_H"

    with open("test_input.h", "w") as f:
        f.write(header_content)

    print("-" * 30)
    print("BASARILI! 'test_input.h' dosyasi olusturuldu.")
    print("Simdi bu dosyayi STM32 projesindeki Core/Inc klasorune kopyala.")
    print("-" * 30)

if __name__ == "__main__":
    generate_header()






question 2

result
<img width="504" height="239" alt="hw5_q2" src="https://github.com/user-attachments/assets/e6d36028-9f3f-46c4-b318-325310b4e2b0" />


training code

import os
import numpy as np
import cv2
import struct # Dosya okuma iÃ§in gerekli
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from matplotlib import pyplot as plt

# --- MANUEL VERÄ° YÃœKLEME FONKSÄ°YONLARI (Hata almamak iÃ§in) ---
def load_images(path):
    with open(path, 'rb') as f:
        magic, num, rows, cols = struct.unpack(">IIII", f.read(16))
        images = np.fromfile(f, dtype=np.uint8).reshape(num, rows, cols)
    return images

def load_labels(path):
    with open(path, 'rb') as f:
        magic, num = struct.unpack(">II", f.read(8))
        labels = np.fromfile(f, dtype=np.uint8)
    return labels
# -----------------------------------------------------------

# Dosya yollarÄ±
train_img_path = os.path.join("train-images.idx3-ubyte")
train_label_path = os.path.join("train-labels.idx1-ubyte")
test_img_path = os.path.join("t10k-images.idx3-ubyte")
test_label_path = os.path.join("t10k-labels.idx1-ubyte")

print("Veriler yÃ¼kleniyor...")
train_images = load_images(train_img_path)
train_labels = load_labels(train_label_path)
test_images = load_images(test_img_path)
test_labels = load_labels(test_label_path)

train_huMoments = np.empty((len(train_images), 7))
test_huMoments = np.empty((len(test_images), 7))

print("Ã–znitelikler (Hu Moments) hesaplanÄ±yor...")
for train_idx, train_img in enumerate(train_images):
    train_moments = cv2.moments(train_img, True)
    train_huMoments[train_idx] = cv2.HuMoments(train_moments).reshape(7)

for test_idx, test_img in enumerate(test_images):
    test_moments = cv2.moments(test_img, True)
    test_huMoments[test_idx] = cv2.HuMoments(test_moments).reshape(7)

# --- Ã–NEMLÄ° EKLEME: NORMALÄ°ZASYON ---
# Kitaptaki Listing 11.6'da bazen atlanmÄ±ÅŸ olsa da, Neural Network'lerin
# Hu momentleri gibi Ã§ok kÃ¼Ã§Ã¼k sayÄ±larla dÃ¼zgÃ¼n Ã§alÄ±ÅŸmasÄ± iÃ§in bu ÅŸarttÄ±r.
features_mean = np.mean(train_huMoments, axis=0)
features_std = np.std(train_huMoments, axis=0)
train_huMoments = (train_huMoments - features_mean) / features_std
test_huMoments = (test_huMoments - features_mean) / features_std

# Model Mimarisi (Kitaptaki gibi: 100 -> 100 -> 10)
model = keras.models.Sequential([
    keras.layers.Dense(100, input_shape=[7], activation="relu"),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.Dense(10, activation="softmax") # 10 sÄ±nÄ±f iÃ§in Softmax
])

# Model Derleme
# SparseCategoricalCrossentropy: Etiketler integer (0,1,2...) olduÄŸu iÃ§in kullanÄ±lÄ±r.
model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),
              optimizer=keras.optimizers.Adam(1e-4), # Learning rate 1e-4
              metrics=['accuracy'])

# Callbacks: En iyi modeli kaydet ve eÄŸitim iyileÅŸmezse erken durdur
mc_callback = ModelCheckpoint("mlp_mnist_model.h5", save_best_only=True)
es_callback = EarlyStopping(monitor="loss", patience=5)

print("Model eÄŸitiliyor...")
# labels olduÄŸu gibi veriliyor (0-9 arasÄ±), binary Ã§evirmeye gerek yok
history = model.fit(train_huMoments, train_labels,
                    epochs=1000, # Early stopping olduÄŸu iÃ§in yÃ¼ksek verilebilir
                    verbose=1,
                    callbacks=[mc_callback, es_callback])

# Tahmin ve SonuÃ§lar
print("Test yapÄ±lÄ±yor...")
nn_preds = model.predict(test_huMoments)
predicted_classes = np.argmax(nn_preds, axis=1) # En yÃ¼ksek olasÄ±lÄ±klÄ± sÄ±nÄ±fÄ± seÃ§

categories = np.unique(test_labels) # 0, 1, 2... 9

conf_matrix = confusion_matrix(test_labels, predicted_classes)
# BÃ¼yÃ¼k matris olduÄŸu iÃ§in gÃ¶rselleÅŸtirmeyi biraz bÃ¼yÃ¼telim
fig, ax = plt.subplots(figsize=(10, 10))
cm_display = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=categories)
cm_display.plot(ax=ax, cmap='viridis') # OkunabilirliÄŸi artÄ±rmak iÃ§in renk haritasÄ±
cm_display.ax_.set_title("Neural Network Confusion Matrix (Multiclass)")
plt.show()

print("Ä°ÅŸlem tamamlandÄ±. 'mlp_mnist_model.h5' kaydedildi.")


h5_to_tflite

import tensorflow as tf

# 1. Modeli yÃ¼kle
model_path = 'mlp_mnist_model.h5'
print(f"Model yukleniyor: {model_path}...")
model = tf.keras.models.load_model(model_path)

# Modelin giriÅŸ boyutunu kontrol edelim (Bunu bir sonraki adÄ±mda kullanacaÄŸÄ±z)
input_shape = model.input_shape
print(f"Model Giris Boyutu: {input_shape}")

# 2. TFLite dÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼yÃ¼ hazÄ±rla
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# 3. Modeli dÃ¶nÃ¼ÅŸtÃ¼r
tflite_model = converter.convert()

# 4. Kaydet
output_path = 'q2_mlp_mnist.tflite'
with open(output_path, 'wb') as f:
    f.write(tflite_model)

print(f"BASARILI! '{output_path}' dosyasi olusturuldu.")


h5_to_c

import os
import numpy as np
import cv2
import struct
import tensorflow as tf
from tensorflow import keras

# --- AYARLAR ---
MODEL_PATH = "mlp_mnist_model.h5"     # Elindeki model dosyasÄ±
OUTPUT_HEADER = "hw5_q2_data.h"        # Ã‡Ä±ktÄ± dosyasÄ±
DATASET_PATH = "."                    # MNIST dosyalarÄ±nÄ±n olduÄŸu klasÃ¶r (aynÄ± dizindeyse nokta)

# --- 1. VERÄ° YÃœKLEME FONKSÄ°YONLARI ---
# (Mean ve Std hesaplamak iÃ§in veriye ihtiyacÄ±mÄ±z var)
def load_images(path):
    with open(path, 'rb') as f:
        magic, num, rows, cols = struct.unpack(">IIII", f.read(16))
        images = np.fromfile(f, dtype=np.uint8).reshape(num, rows, cols)
    return images

def extract_hu_moments(images):
    print(f"   -> {len(images)} resimden Hu Momentler Ã§Ä±karÄ±lÄ±yor...")
    hu_list = np.empty((len(images), 7))
    for idx, img in enumerate(images):
        moments = cv2.moments(img, True)
        hu_list[idx] = cv2.HuMoments(moments).reshape(7)
    return hu_list

# --- 2. ANA Ä°ÅžLEM ---
def export_model_to_c():
    print("1. MNIST EÄŸitim verisi yÃ¼kleniyor (Mean/Std hesaplamak iÃ§in)...")
    train_img_path = os.path.join(DATASET_PATH, "train-images.idx3-ubyte")
    
    if not os.path.exists(train_img_path):
        print(f"HATA: {train_img_path} bulunamadÄ±! Mean/Std hesaplanamÄ±yor.")
        return

    train_images = load_images(train_img_path)
    
    # Hu Momentleri hesapla
    train_huMoments = extract_hu_moments(train_images)
    
    # Mean ve Std Hesapla (EÄŸitimdeki aynÄ± iÅŸlemi tekrarlÄ±yoruz)
    features_mean = np.mean(train_huMoments, axis=0)
    features_std = np.std(train_huMoments, axis=0)
    
    print("\n2. Normalizasyon Parametreleri HesaplandÄ±:")
    print(f"   Mean: {features_mean}")
    print(f"   Std : {features_std}")

    # --- MODELÄ° YÃœKLE ---
    print(f"\n3. Model YÃ¼kleniyor: {MODEL_PATH}")
    if not os.path.exists(MODEL_PATH):
        print("HATA: .h5 dosyasÄ± bulunamadÄ±!")
        return
        
    model = keras.models.load_model(MODEL_PATH)
    print("   Model baÅŸarÄ±yla yÃ¼klendi.")

    # --- C HEADER OLUÅžTUR ---
    print(f"\n4. C Header dosyasÄ± oluÅŸturuluyor: {OUTPUT_HEADER}")
    with open(OUTPUT_HEADER, "w") as f:
        f.write("/* STM32 Neural Network Model Data */\n")
        f.write("#ifndef MODEL_DATA_H\n#define MODEL_DATA_H\n\n")
        
        # A) Normalizasyon DeÄŸerlerini Yaz
        f.write("// --- Normalization Parameters (Mean & Std) ---\n")
        f.write(f"const float features_mean[7] = {{\n")
        for val in features_mean:
            f.write(f"    {val:.6f}f, \n")
        f.write("};\n\n")

        f.write(f"const float features_std[7] = {{\n")
        for val in features_std:
            f.write(f"    {val:.6f}f, \n")
        f.write("};\n\n")

        # B) AÄŸÄ±rlÄ±klar ve Biaslar
        f.write("// --- Model Weights & Biases ---\n")
        for i, layer in enumerate(model.layers):
            weights = layer.get_weights()
            if len(weights) > 0: # Sadece aÄŸÄ±rlÄ±ÄŸÄ± olan katmanlar (Dense)
                w, b = weights
                
                # Weights (Flatten edilmiÅŸ)
                f.write(f"// Layer {i} ({layer.name}) Weights - Shape: {w.shape}\n")
                f.write(f"const float layer{i}_weights[] = {{\n")
                for val in w.flatten():
                    f.write(f"{val:.6f}f, ")
                f.write("};\n\n")
                
                # Biases
                f.write(f"// Layer {i} ({layer.name}) Biases - Shape: {b.shape}\n")
                f.write(f"const float layer{i}_biases[] = {{\n")
                for val in b.flatten():
                    f.write(f"{val:.6f}f, ")
                f.write("};\n\n")
        
        f.write("#endif // MODEL_DATA_H\n")
    
    print(f"\nBAÅžARILI! '{OUTPUT_HEADER}' dosyasÄ± oluÅŸturuldu.")
    print("Bu dosyayÄ± STM32 projenin 'Core/Inc' klasÃ¶rÃ¼ne kopyalayabilirsin.")

if __name__ == "__main__":
    export_model_to_c()


test_data_generate

import os
import numpy as np
import cv2
import struct
import random

# --- DOSYA YOLLARI ---
# EÄŸer dosyalar aynÄ± klasÃ¶rdeyse "." kalsÄ±n
DATASET_PATH = "." 
OUTPUT_HEADER = "hw5_q2_test_data.h"
NUM_SAMPLES = 20  # KaÃ§ tane test verisi gÃ¶mmek istiyorsun?

def load_images(path):
    with open(path, 'rb') as f:
        magic, num, rows, cols = struct.unpack(">IIII", f.read(16))
        images = np.fromfile(f, dtype=np.uint8).reshape(num, rows, cols)
    return images

def load_labels(path):
    with open(path, 'rb') as f:
        magic, num = struct.unpack(">II", f.read(8))
        labels = np.fromfile(f, dtype=np.uint8)
    return labels

def create_test_header():
    print("1. MNIST Test verileri yÃ¼kleniyor...")
    img_path = os.path.join(DATASET_PATH, "t10k-images.idx3-ubyte")
    lbl_path = os.path.join(DATASET_PATH, "t10k-labels.idx1-ubyte")
    
    if not os.path.exists(img_path):
        print(f"HATA: {img_path} bulunamadÄ±!")
        return

    test_images = load_images(img_path)
    test_labels = load_labels(lbl_path)
    
    # Rastgele seÃ§im yapmayalÄ±m, her seferinde aynÄ±larÄ±nÄ± alalÄ±m ki kÄ±yaslamasÄ± kolay olsun (ilk N tanesi)
    # Ä°stersen random.sample kullanabilirsin ama sÄ±ralÄ± gitmek debug iÃ§in iyidir.
    indices = range(NUM_SAMPLES) 
    
    print(f"2. Ä°lk {NUM_SAMPLES} verinin Hu Momentleri hesaplanÄ±yor...")
    
    with open(OUTPUT_HEADER, "w") as f:
        f.write(f"#ifndef TEST_DATA_H\n#define TEST_DATA_H\n\n")
        f.write(f"// MNIST Test Setinden ilk {NUM_SAMPLES} ornek\n")
        f.write(f"#define NUM_TEST_SAMPLES {NUM_SAMPLES}\n\n")
        
        # 1. Labels (GerÃ§ek Cevaplar)
        f.write(f"const int test_labels[{NUM_SAMPLES}] = {{\n    ")
        for i in indices:
            f.write(f"{test_labels[i]}, ")
        f.write("\n};\n\n")
        
        # 2. Inputs (Raw Hu Moments)
        f.write(f"const float test_samples[{NUM_SAMPLES}][7] = {{\n")
        
        for i in indices:
            img = test_images[i]
            moments = cv2.moments(img, True)
            hu = cv2.HuMoments(moments).reshape(7)
            
            f.write("    {")
            for val in hu:
                f.write(f"{val:.6f}f, ")
            f.write("}, \n")
            
        f.write("};\n\n")
        f.write("#endif // TEST_DATA_H\n")
        
    print(f"BAÅžARILI! '{OUTPUT_HEADER}' oluÅŸturuldu.")
    print("Bu dosyayÄ± 'hw5_q2_data.h' yanÄ±na (Core/Inc klasÃ¶rÃ¼ne) kopyala.")

if __name__ == "__main__":
    create_test_header()


app_x-cube-ai.c


/**
  ******************************************************************************
  * @file    app_x-cube-ai.c
  * @author  X-CUBE-AI C code generator
  * @brief   AI program body
  ******************************************************************************
  * @attention
  *
  * Copyright (c) 2026 STMicroelectronics.
  * All rights reserved.
  *
  * This software is licensed under terms that can be found in the LICENSE file
  * in the root directory of this software component.
  * If no LICENSE file comes with this software, it is provided AS-IS.
  *
  ******************************************************************************
  */

 /*
  * Description
  *   v1.0 - Minimum template to show how to use the Embedded Client API
  *          model. Only one input and one output is supported. All
  *          memory resources are allocated statically (AI_NETWORK_XX, defines
  *          are used).
  *          Re-target of the printf function is out-of-scope.
  *   v2.0 - add multiple IO and/or multiple heap support
  *
  *   For more information, see the embeded documentation:
  *
  *       [1] %X_CUBE_AI_DIR%/Documentation/index.html
  *
  *   X_CUBE_AI_DIR indicates the location where the X-CUBE-AI pack is installed
  *   typical : C:\Users\[user_name]\STM32Cube\Repository\STMicroelectronics\X-CUBE-AI\7.1.0
  */

#ifdef __cplusplus
 extern "C" {
#endif

/* Includes ------------------------------------------------------------------*/

#if defined ( __ICCARM__ )
#elif defined ( __CC_ARM ) || ( __GNUC__ )
#endif

/* System headers */
#include <stdint.h>
#include <stdlib.h>
#include <stdio.h>
#include <inttypes.h>
#include <string.h>

#include "app_x-cube-ai.h"
#include "main.h"
#include "ai_datatypes_defines.h"
#include "hw5_q2_network.h"
#include "hw5_q2_network_data.h"

/* USER CODE BEGIN includes */
#include "test_input_image.h"
 float max_prob = 0.0f;
 int predicted_digit = -1;
/* USER CODE END includes */

/* IO buffers ----------------------------------------------------------------*/

#if !defined(AI_HW5_Q2_NETWORK_INPUTS_IN_ACTIVATIONS)
AI_ALIGNED(4) ai_i8 data_in_1[AI_HW5_Q2_NETWORK_IN_1_SIZE_BYTES];
ai_i8* data_ins[AI_HW5_Q2_NETWORK_IN_NUM] = {
data_in_1
};
#else
ai_i8* data_ins[AI_HW5_Q2_NETWORK_IN_NUM] = {
NULL
};
#endif

#if !defined(AI_HW5_Q2_NETWORK_OUTPUTS_IN_ACTIVATIONS)
AI_ALIGNED(4) ai_i8 data_out_1[AI_HW5_Q2_NETWORK_OUT_1_SIZE_BYTES];
ai_i8* data_outs[AI_HW5_Q2_NETWORK_OUT_NUM] = {
data_out_1
};
#else
ai_i8* data_outs[AI_HW5_Q2_NETWORK_OUT_NUM] = {
NULL
};
#endif

/* Activations buffers -------------------------------------------------------*/

AI_ALIGNED(32)
static uint8_t pool0[AI_HW5_Q2_NETWORK_DATA_ACTIVATION_1_SIZE];

ai_handle data_activations0[] = {pool0};

/* AI objects ----------------------------------------------------------------*/

static ai_handle hw5_q2_network = AI_HANDLE_NULL;

static ai_buffer* ai_input;
static ai_buffer* ai_output;

static void ai_log_err(const ai_error err, const char *fct)
{
  /* USER CODE BEGIN log */
  if (fct)
    printf("TEMPLATE - Error (%s) - type=0x%02x code=0x%02x\r\n", fct,
        err.type, err.code);
  else
    printf("TEMPLATE - Error - type=0x%02x code=0x%02x\r\n", err.type, err.code);

  do {} while (1);
  /* USER CODE END log */
}

static int ai_boostrap(ai_handle *act_addr)
{
  ai_error err;

  /* Create and initialize an instance of the model */
  err = ai_hw5_q2_network_create_and_init(&hw5_q2_network, act_addr, NULL);
  if (err.type != AI_ERROR_NONE) {
    ai_log_err(err, "ai_hw5_q2_network_create_and_init");
    return -1;
  }

  ai_input = ai_hw5_q2_network_inputs_get(hw5_q2_network, NULL);
  ai_output = ai_hw5_q2_network_outputs_get(hw5_q2_network, NULL);

#if defined(AI_HW5_Q2_NETWORK_INPUTS_IN_ACTIVATIONS)
  /*  In the case where "--allocate-inputs" option is used, memory buffer can be
   *  used from the activations buffer. This is not mandatory.
   */
  for (int idx=0; idx < AI_HW5_Q2_NETWORK_IN_NUM; idx++) {
	data_ins[idx] = ai_input[idx].data;
  }
#else
  for (int idx=0; idx < AI_HW5_Q2_NETWORK_IN_NUM; idx++) {
	  ai_input[idx].data = data_ins[idx];
  }
#endif

#if defined(AI_HW5_Q2_NETWORK_OUTPUTS_IN_ACTIVATIONS)
  /*  In the case where "--allocate-outputs" option is used, memory buffer can be
   *  used from the activations buffer. This is no mandatory.
   */
  for (int idx=0; idx < AI_HW5_Q2_NETWORK_OUT_NUM; idx++) {
	data_outs[idx] = ai_output[idx].data;
  }
#else
  for (int idx=0; idx < AI_HW5_Q2_NETWORK_OUT_NUM; idx++) {
	ai_output[idx].data = data_outs[idx];
  }
#endif

  return 0;
}

static int ai_run(void)
{
  ai_i32 batch;

  batch = ai_hw5_q2_network_run(hw5_q2_network, ai_input, ai_output);
  if (batch != 1) {
    ai_log_err(ai_hw5_q2_network_get_error(hw5_q2_network),
        "ai_hw5_q2_network_run");
    return -1;
  }

  return 0;
}

/* USER CODE BEGIN 2 */
int acquire_and_process_data(ai_i8* data[])
{
  /* fill the inputs of the c-model
  for (int idx=0; idx < AI_HW5_Q2_NETWORK_IN_NUM; idx++ )
  {
      data[idx] = ....
  }

  */
  return 0;
}

int post_process(ai_i8* data[])
{
  /* process the predictions
  for (int idx=0; idx < AI_HW5_Q2_NETWORK_OUT_NUM; idx++ )
  {
      data[idx] = ....
  }

  */
  return 0;
}
/* USER CODE END 2 */

/* Entry points --------------------------------------------------------------*/

void MX_X_CUBE_AI_Init(void)
{
    /* USER CODE BEGIN 5 */
  printf("\r\nTEMPLATE - initialization\r\n");

  ai_boostrap(data_activations0);
    /* USER CODE END 5 */
}

void MX_X_CUBE_AI_Process(void)
{
    /* USER CODE BEGIN 6 */
	    int res = -1;

	    // 1. Model HazÄ±r mÄ± Kontrol Et
	    if (hw5_q2_network) { // (Senin model degiskenin farkliysa duzelt, genelde hw5_last veya network olur)

	        // Giris ve Cikis Hafiza Adreslerini Al
	        ai_float *in_data = (ai_float *)ai_input[0].data;
	        ai_float *out_data = (ai_float *)ai_output[0].data;

	        // ---------------------------------------------
	        // SORU 2: MNIST RAKAM TANIMA (Giris: 7 Hu Moment)
	        // ---------------------------------------------

	        // 2. Veriyi Yukle (test_input.h dosyasindan)
	        // DÄ°KKAT: DÃ¶ngÃ¼ sayÄ±sÄ± artÄ±k 7!
	        for (int i = 0; i < 7; i++) {
	            in_data[i] = (ai_float)test_input_image[i];
	        }

	        // 3. Modeli Calistir
	        res = ai_run();

	        // 4. Sonucu Analiz Et (En yuksek olasilik hangisi?)
	        if (res == 0) {

	            // Global degiskenleri sifirla (Debug ekrani icin)
	            max_prob = 0.0f;
	            predicted_digit = -1;

	            // 0'dan 9'a kadar tum cikislara bak
	            for (int i = 0; i < 10; i++) {
	                if (out_data[i] > max_prob) {
	                    max_prob = out_data[i];
	                    predicted_digit = i;
	                }
	            }
	        }

	        // Islemciyi yormamak icin bekle
	        HAL_Delay(2000);
	    }

	    if (res) {
	        ai_error err = {AI_ERROR_INVALID_STATE, AI_ERROR_CODE_NETWORK};
	        ai_log_err(err, "Process has FAILED");
	    }
    /* USER CODE END 6 */
}
#ifdef __cplusplus
}
#endif


